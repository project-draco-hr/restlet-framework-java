def optionflags():
    '\nTests of `DocTestRunner`\'s option flag handling.\n\nSeveral option flags can be used to customize the behavior of the test\nrunner.  These are defined as module constants in doctest, and passed\nto the DocTestRunner constructor (multiple constants should be or-ed\ntogether).\n\nThe DONT_ACCEPT_TRUE_FOR_1 flag disables matches between True/False\nand 1/0:\n\n    >>> def f(x):\n    ...     \'>>> True\\n1\\n\'\n\n    >>> # Without the flag:\n    >>> test = doctest.DocTestFinder().find(f)[0]\n    >>> doctest.DocTestRunner(verbose=False).run(test)\n    (0, 1)\n\n    >>> # With the flag:\n    >>> test = doctest.DocTestFinder().find(f)[0]\n    >>> flags = doctest.DONT_ACCEPT_TRUE_FOR_1\n    >>> doctest.DocTestRunner(verbose=False, optionflags=flags).run(test)\n    ... # doctest: +ELLIPSIS\n    **********************************************************************\n    File ..., line 2, in f\n    Failed example:\n        True\n    Expected:\n        1\n    Got:\n        True\n    (1, 1)\n\nThe DONT_ACCEPT_BLANKLINE flag disables the match between blank lines\nand the \'<BLANKLINE>\' marker:\n\n    >>> def f(x):\n    ...     \'>>> print "a\\\\n\\\\nb"\\na\\n<BLANKLINE>\\nb\\n\'\n\n    >>> # Without the flag:\n    >>> test = doctest.DocTestFinder().find(f)[0]\n    >>> doctest.DocTestRunner(verbose=False).run(test)\n    (0, 1)\n\n    >>> # With the flag:\n    >>> test = doctest.DocTestFinder().find(f)[0]\n    >>> flags = doctest.DONT_ACCEPT_BLANKLINE\n    >>> doctest.DocTestRunner(verbose=False, optionflags=flags).run(test)\n    ... # doctest: +ELLIPSIS\n    **********************************************************************\n    File ..., line 2, in f\n    Failed example:\n        print "a\\n\\nb"\n    Expected:\n        a\n        <BLANKLINE>\n        b\n    Got:\n        a\n    <BLANKLINE>\n        b\n    (1, 1)\n\nThe NORMALIZE_WHITESPACE flag causes all sequences of whitespace to be\ntreated as equal:\n\n    >>> def f(x):\n    ...     \'>>> print 1, 2, 3\\n  1   2\\n 3\'\n\n    >>> # Without the flag:\n    >>> test = doctest.DocTestFinder().find(f)[0]\n    >>> doctest.DocTestRunner(verbose=False).run(test)\n    ... # doctest: +ELLIPSIS\n    **********************************************************************\n    File ..., line 2, in f\n    Failed example:\n        print 1, 2, 3\n    Expected:\n          1   2\n         3\n    Got:\n        1 2 3\n    (1, 1)\n\n    >>> # With the flag:\n    >>> test = doctest.DocTestFinder().find(f)[0]\n    >>> flags = doctest.NORMALIZE_WHITESPACE\n    >>> doctest.DocTestRunner(verbose=False, optionflags=flags).run(test)\n    (0, 1)\n\n    An example from the docs:\n    >>> print range(20) #doctest: +NORMALIZE_WHITESPACE\n    [0,   1,  2,  3,  4,  5,  6,  7,  8,  9,\n    10,  11, 12, 13, 14, 15, 16, 17, 18, 19]\n\nThe ELLIPSIS flag causes ellipsis marker ("...") in the expected\noutput to match any substring in the actual output:\n\n    >>> def f(x):\n    ...     \'>>> print range(15)\\n[0, 1, 2, ..., 14]\\n\'\n\n    >>> # Without the flag:\n    >>> test = doctest.DocTestFinder().find(f)[0]\n    >>> doctest.DocTestRunner(verbose=False).run(test)\n    ... # doctest: +ELLIPSIS\n    **********************************************************************\n    File ..., line 2, in f\n    Failed example:\n        print range(15)\n    Expected:\n        [0, 1, 2, ..., 14]\n    Got:\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n    (1, 1)\n\n    >>> # With the flag:\n    >>> test = doctest.DocTestFinder().find(f)[0]\n    >>> flags = doctest.ELLIPSIS\n    >>> doctest.DocTestRunner(verbose=False, optionflags=flags).run(test)\n    (0, 1)\n\n    ... also matches nothing:\n\n    >>> for i in range(100):\n    ...     print i**2, #doctest: +ELLIPSIS\n    0 1...4...9 16 ... 36 49 64 ... 9801\n\n    ... can be surprising; e.g., this test passes:\n\n    >>> for i in range(21): #doctest: +ELLIPSIS\n    ...     print i,\n    0 1 2 ...1...2...0\n\n    Examples from the docs:\n\n    >>> print range(20) # doctest:+ELLIPSIS\n    [0, 1, ..., 18, 19]\n\n    >>> print range(20) # doctest: +ELLIPSIS\n    ...                 # doctest: +NORMALIZE_WHITESPACE\n    [0,    1, ...,   18,    19]\n\nThe SKIP flag causes an example to be skipped entirely.  I.e., the\nexample is not run.  It can be useful in contexts where doctest\nexamples serve as both documentation and test cases, and an example\nshould be included for documentation purposes, but should not be\nchecked (e.g., because its output is random, or depends on resources\nwhich would be unavailable.)  The SKIP flag can also be used for\n\'commenting out\' broken examples.\n\n    >>> import unavailable_resource           # doctest: +SKIP\n    >>> unavailable_resource.do_something()   # doctest: +SKIP\n    >>> unavailable_resource.blow_up()        # doctest: +SKIP\n    Traceback (most recent call last):\n        ...\n    UncheckedBlowUpError:  Nobody checks me.\n\n    >>> import random\n    >>> print random.random() # doctest: +SKIP\n    0.721216923889\n\nThe REPORT_UDIFF flag causes failures that involve multi-line expected\nand actual outputs to be displayed using a unified diff:\n\n    >>> def f(x):\n    ...     r\'\'\'\n    ...     >>> print \'\\n\'.join(\'abcdefg\')\n    ...     a\n    ...     B\n    ...     c\n    ...     d\n    ...     f\n    ...     g\n    ...     h\n    ...     \'\'\'\n\n    >>> # Without the flag:\n    >>> test = doctest.DocTestFinder().find(f)[0]\n    >>> doctest.DocTestRunner(verbose=False).run(test)\n    ... # doctest: +ELLIPSIS\n    **********************************************************************\n    File ..., line 3, in f\n    Failed example:\n        print \'\\n\'.join(\'abcdefg\')\n    Expected:\n        a\n        B\n        c\n        d\n        f\n        g\n        h\n    Got:\n        a\n        b\n        c\n        d\n        e\n        f\n        g\n    (1, 1)\n\n    >>> # With the flag:\n    >>> test = doctest.DocTestFinder().find(f)[0]\n    >>> flags = doctest.REPORT_UDIFF\n    >>> doctest.DocTestRunner(verbose=False, optionflags=flags).run(test)\n    ... # doctest: +ELLIPSIS\n    **********************************************************************\n    File ..., line 3, in f\n    Failed example:\n        print \'\\n\'.join(\'abcdefg\')\n    Differences (unified diff with -expected +actual):\n        @@ -1,7 +1,7 @@\n         a\n        -B\n        +b\n         c\n         d\n        +e\n         f\n         g\n        -h\n    (1, 1)\n\nThe REPORT_CDIFF flag causes failures that involve multi-line expected\nand actual outputs to be displayed using a context diff:\n\n    >>> # Reuse f() from the REPORT_UDIFF example, above.\n    >>> test = doctest.DocTestFinder().find(f)[0]\n    >>> flags = doctest.REPORT_CDIFF\n    >>> doctest.DocTestRunner(verbose=False, optionflags=flags).run(test)\n    ... # doctest: +ELLIPSIS\n    **********************************************************************\n    File ..., line 3, in f\n    Failed example:\n        print \'\\n\'.join(\'abcdefg\')\n    Differences (context diff with expected followed by actual):\n        ***************\n        *** 1,7 ****\n          a\n        ! B\n          c\n          d\n          f\n          g\n        - h\n        --- 1,7 ----\n          a\n        ! b\n          c\n          d\n        + e\n          f\n          g\n    (1, 1)\n\n\nThe REPORT_NDIFF flag causes failures to use the difflib.Differ algorithm\nused by the popular ndiff.py utility.  This does intraline difference\nmarking, as well as interline differences.\n\n    >>> def f(x):\n    ...     r\'\'\'\n    ...     >>> print "a b  c d e f g h i   j k l m"\n    ...     a b c d e f g h i j k 1 m\n    ...     \'\'\'\n    >>> test = doctest.DocTestFinder().find(f)[0]\n    >>> flags = doctest.REPORT_NDIFF\n    >>> doctest.DocTestRunner(verbose=False, optionflags=flags).run(test)\n    ... # doctest: +ELLIPSIS\n    **********************************************************************\n    File ..., line 3, in f\n    Failed example:\n        print "a b  c d e f g h i   j k l m"\n    Differences (ndiff with -expected +actual):\n        - a b c d e f g h i j k 1 m\n        ?                       ^\n        + a b  c d e f g h i   j k l m\n        ?     +              ++    ^\n    (1, 1)\n\nThe REPORT_ONLY_FIRST_FAILURE supresses result output after the first\nfailing example:\n\n    >>> def f(x):\n    ...     r\'\'\'\n    ...     >>> print 1 # first success\n    ...     1\n    ...     >>> print 2 # first failure\n    ...     200\n    ...     >>> print 3 # second failure\n    ...     300\n    ...     >>> print 4 # second success\n    ...     4\n    ...     >>> print 5 # third failure\n    ...     500\n    ...     \'\'\'\n    >>> test = doctest.DocTestFinder().find(f)[0]\n    >>> flags = doctest.REPORT_ONLY_FIRST_FAILURE\n    >>> doctest.DocTestRunner(verbose=False, optionflags=flags).run(test)\n    ... # doctest: +ELLIPSIS\n    **********************************************************************\n    File ..., line 5, in f\n    Failed example:\n        print 2 # first failure\n    Expected:\n        200\n    Got:\n        2\n    (3, 5)\n\nHowever, output from `report_start` is not supressed:\n\n    >>> doctest.DocTestRunner(verbose=True, optionflags=flags).run(test)\n    ... # doctest: +ELLIPSIS\n    Trying:\n        print 1 # first success\n    Expecting:\n        1\n    ok\n    Trying:\n        print 2 # first failure\n    Expecting:\n        200\n    **********************************************************************\n    File ..., line 5, in f\n    Failed example:\n        print 2 # first failure\n    Expected:\n        200\n    Got:\n        2\n    (3, 5)\n\nFor the purposes of REPORT_ONLY_FIRST_FAILURE, unexpected exceptions\ncount as failures:\n\n    >>> def f(x):\n    ...     r\'\'\'\n    ...     >>> print 1 # first success\n    ...     1\n    ...     >>> raise ValueError(2) # first failure\n    ...     200\n    ...     >>> print 3 # second failure\n    ...     300\n    ...     >>> print 4 # second success\n    ...     4\n    ...     >>> print 5 # third failure\n    ...     500\n    ...     \'\'\'\n    >>> test = doctest.DocTestFinder().find(f)[0]\n    >>> flags = doctest.REPORT_ONLY_FIRST_FAILURE\n    >>> doctest.DocTestRunner(verbose=False, optionflags=flags).run(test)\n    ... # doctest: +ELLIPSIS\n    **********************************************************************\n    File ..., line 5, in f\n    Failed example:\n        raise ValueError(2) # first failure\n    Exception raised:\n        ...\n        ValueError: 2\n    (3, 5)\n\nNew option flags can also be registered, via register_optionflag().  Here\nwe reach into doctest\'s internals a bit.\n\n    >>> unlikely = "UNLIKELY_OPTION_NAME"\n    >>> unlikely in doctest.OPTIONFLAGS_BY_NAME\n    False\n    >>> new_flag_value = doctest.register_optionflag(unlikely)\n    >>> unlikely in doctest.OPTIONFLAGS_BY_NAME\n    True\n\nBefore 2.4.4/2.5, registering a name more than once erroneously created\nmore than one flag value.  Here we verify that\'s fixed:\n\n    >>> redundant_flag_value = doctest.register_optionflag(unlikely)\n    >>> redundant_flag_value == new_flag_value\n    True\n\nClean up.\n    >>> del doctest.OPTIONFLAGS_BY_NAME[unlikely]\n\n    '
